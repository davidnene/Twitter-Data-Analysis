{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Mining Project_Challenge_ Day2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidnene/Twitter-Data-Analysis/blob/main/Data_Mining_Project_Challenge__Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDwep1K8Erxl"
      },
      "source": [
        "**Project:** Data Minining Project for  X company"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7-ii3uyI8KY"
      },
      "source": [
        "The CRISP-DM Framework\n",
        "\n",
        "\n",
        "The CRISP-DM methodology provides a structured approach to planning a data mining project. It is a robust and well-proven methodology.\n",
        "* Business understanding (BU): Determine Business Objectives, Assess Situation, Determine Data Mining Goals, Produce Project Plan\n",
        "\n",
        "* Data understanding (DU): Collect Initial Data, Describe Data, Explore Data, Verify Data Quality\n",
        "\n",
        "* Data preparation (DP): Select Data, Clean Data, Construct Data, Integrate Data\n",
        "\n",
        "* Modeling (M): Select modeling technique, Generate Test Design, Build Model, Assess Model\n",
        "*  Evaluation (E): Evaluate Results, Review Process, Determine Next Steps\n",
        "*  Deployment (D): Plan Deployment, Plan Monitoring and Maintenance, Produce Final Report, Review Project\n",
        "\n",
        "\n",
        "References:\n",
        "\n",
        "[What is the CRISP-DM methodology?](https://www.sv-europe.com/crisp-dm-methodology/)\n",
        "\n",
        "[Introduction to CRISP DM Framework for Data Science and Machine Learning](https://www.linkedin.com/pulse/chapter-1-introduction-crisp-dm-framework-data-science-anshul-roy/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lo7Ml7tMQOf"
      },
      "source": [
        "**Data Set**\n",
        "### The data is for company X which is trying to control attrition. \n",
        "### There are two sets of data: \"Existing employees\" and \"Employees who have left\". The following attributes are available for every employee.\n",
        "\n",
        "\n",
        "*   Satisfaction Level\n",
        "\n",
        "*   Last evaluation\n",
        "\n",
        "*   Number of projects\n",
        "\n",
        "*   Average monthly hours\n",
        "\n",
        "*   Time spent at the company\n",
        "*   Whether they have had a work accident\n",
        "\n",
        "\n",
        "*  Whether they have had a promotion in the last 5 years\n",
        "\n",
        "\n",
        "*   Departments (column sales)\n",
        "\n",
        "\n",
        "*   Salary\n",
        "\n",
        "\n",
        "*  Whether the employee has left\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjSj2A2sSph_"
      },
      "source": [
        "**Your Role**\n",
        " \n",
        "\n",
        "*   As data science team member X company asked you to answer this two questions.\n",
        "*  What type of employees is leaving? \n",
        "\n",
        "*   Determine which employees are prone to leave next.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajdEVA7LiBUp"
      },
      "source": [
        "Business Understanding\n",
        "\n",
        "---\n",
        "\n",
        "This step mostly focuses on understanding the Business in all the different aspects. It follows the below different steps.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Identify the goal and frame the business problem.\n",
        "* Prepare Analytical Goal i.e. what type of performance metric and loss function to use\n",
        "* Gather information on resource, constraints, assumptions, risks etc\n",
        "* Gather information on resource, constraints, assumptions, risks etc\n",
        "*   Prepare Work Flow Chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4MwiCYzj2_u"
      },
      "source": [
        "### Write the main objectives of this project in your words?\n",
        "minimum of 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STyLda45j1Mf"
      },
      "source": [
        "main_objectives = '''1. To do data preparation and Exploratory Data Analysis so as \n",
        "                       to understand the data and determine what type of employee is leaving\n",
        "                     2. To build a efficiently working ML model that can be used to make predictions\n",
        "                       what type of employee is prone to leaving based on new incoming data\n",
        "                  '''"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuOlxLxKMOLI"
      },
      "source": [
        "assert len(main_objectives) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(main_objectives) > 80 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyXeNxlCkbaw"
      },
      "source": [
        "### Outline the different data analysis steps you will follow to carry out the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC-tl8sUksQq"
      },
      "source": [
        "dm_outline = '''1. Univariate analysis to understand the important variables\n",
        "                2. Multivariate analysis to understand relationships between variables in relation to the output.\n",
        "                    eg.correlation\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K1mWuDoksTk"
      },
      "source": [
        "assert len(dm_outline) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(dm_outline) > 70 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmUDFG1wkzUy"
      },
      "source": [
        "### What metrics will you use to measure the performance of your data analysis model? \n",
        "Write the equations of the metrics here\n",
        "\n",
        "**Confusion Matrix**\n",
        "\n",
        "[[TP FP]\n",
        " \n",
        " [FN TN]]\n",
        "\n",
        " **Accuracy**\n",
        "\n",
        " $\\frac{(TP+TN)}{total}$ \n",
        "\n",
        " \n",
        " **Sensitivity**\n",
        " \n",
        " $\\frac{TP}{actualyes}$\n",
        "\n",
        "**F-Score**\n",
        "\n",
        "$\\frac{tp}{(tp + 1/2(fp+fn)}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCNulojKk_BP"
      },
      "source": [
        "e.g. Precision = $\\frac{TP}{(TP + FP)}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLS2YHoRk_EK"
      },
      "source": [
        "Why do you choose these metrics? minimum of 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSynT14KlPSJ"
      },
      "source": [
        "why_metrics = '''Add your answer text here\n",
        "you can create python string using (') or (\") or 3('), like the text here. The 3(') string can be used \n",
        "to write paragraphs, comments in the beginning of functions, etc.. Your answer to the above question \n",
        "should replace this text.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr-Mk0E8lPVJ"
      },
      "source": [
        "assert len(why_metrics) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(why_metrics) > 80 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAo19Ip6lUtm"
      },
      "source": [
        "### How would you know if your data analysis work is a success or not?\n",
        "minimum of 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HESsiXW5llX-"
      },
      "source": [
        "how_success = '''Add your answer text here\n",
        "you can create python string using (') or (\") or 3('), like the text here. The 3(') string can be used \n",
        "to write paragraphs, comments in the beginning of functions, etc.. Your answer to the above question \n",
        "should replace this text.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdUoiMIOlmXq"
      },
      "source": [
        "assert len(how_success) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(how_success) > 80 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQE6dqo6l1TZ"
      },
      "source": [
        "## What kind of challenges do you expect in your analysis?\n",
        "List at least 3 challenges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrAhBQhQl8Lh"
      },
      "source": [
        "challenge_text = '''Add your answer text here\n",
        "you can create python string using (') or (\") or 3('), like the text here. The 3(') string can be used \n",
        "to write paragraphs, comments in the beginning of functions, etc.. Your answer to the above question \n",
        "should replace this text.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EedHa-Pll8X7"
      },
      "source": [
        "assert len(challenge_text) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(how_success) > 80 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcJ8M6uWDeSE"
      },
      "source": [
        "<h2>Using the processed twitter data from yesterday's challenge</h2>.\n",
        "\n",
        "\n",
        "- Form a new data frame (named `cleanTweet`), containing columns $\\textbf{clean-text}$ and $\\textbf{polarity}$.\n",
        "\n",
        "- Write a function `text_category` that takes a value `p` and returns, depending on the value of p, a string `'positive'`, `'negative'` or `'neutral'`.\n",
        "\n",
        "- Apply this function (`text_category`) on the $\\textbf{polarity}$ column of `cleanTweet` in 1 above to form a new column called $\\textbf{score}$ in `cleanTweet`.\n",
        "\n",
        "- Visualize The $\\textbf{score}$ column using piechart and barchart\n",
        "\n",
        "<h5>Now we want to build a classification model on the clean tweet following the steps below:</h5>\n",
        "\n",
        "* Remove rows from `cleanTweet` where $\\textbf{polarity}$ $= 0$ (i.e where $\\textbf{score}$ = Neutral) and reset the frame index.\n",
        "* Construct a column $\\textbf{scoremap}$ Use the mapping {'positive':1, 'negative':0} on the $\\textbf{score}$ column\n",
        "* Create feature and target variables `(X,y)` from $\\textbf{clean-text}$ and $\\textbf{scoremap}$ columns respectively.\n",
        "* Use `train_test_split` function to construct `(X_train, y_train)` and `(X_test, y_test)` from `(X,y)`\n",
        "\n",
        "* Build an `SGDClassifier` model from the vectorize train text data. Use `CountVectorizer()` with a $\\textit{trigram}$ parameter.\n",
        "\n",
        "* Evaluate your model on the test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85WxmGNGDcBY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnTccpxNj8yu",
        "outputId": "6f8b5d46-acdf-4525-e6a1-4b5974716768"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu6AvZZQkQxQ"
      },
      "source": [
        "#importing prerequisite libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "VEbzeJIokYXV",
        "outputId": "9fb1cf04-35e2-4db6-b75f-f09f95458320"
      },
      "source": [
        "#loading the data\n",
        "\n",
        "employees_df = pd.read_csv('/content/cleaned_fintech_data.csv')\n",
        "employees_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9439b9be3965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loading the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0memployees_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://drive.google.com/file/d/16f1PIEznO6Qhw5EHJzRgbP-rvDuHkJkj/view?usp=sharing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0memployees_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 301 fields in line 132, saw 440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3TM6sxWkqSA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}